# Seasonal Jobs Platform

A comprehensive ETL pipeline and analytics platform for processing and visualizing H2 seasonal job opportunities from the U.S. Department of Labor.

## üìã Overview

This project implements an **incremental ETL pipeline** that:
- Extracts seasonal job data from the Department of Labor API
- Transforms and cleans the data
- Stores versioned datasets in Parquet format
- Serves interactive analytics via Streamlit

## üèóÔ∏è Application Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SEASONAL JOBS PLATFORM                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                        DATA FLOW DIAGRAM

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  External Data      ‚îÇ
‚îÇ  Source (DOL API)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ https://api.seasonaljobs.dol.gov/datahub/
           ‚îÇ (api-version: 2020-06-30)
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EXTRACTION LAYER (extract_seasonal_jobs.py)                    ‚îÇ
‚îÇ  ‚úì Paginated API requests with retry strategy                   ‚îÇ
‚îÇ  ‚úì Checkpoint-based incremental loading                         ‚îÇ
‚îÇ  ‚úì Deduplication by case_id                                     ‚îÇ
‚îÇ  ‚úì Output: seasonal_jobs_raw.parquet                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Stores checkpoint timestamps
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DATASET LAYER                                                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ seasonal_jobs_raw.parquet      (Raw data + history)        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ seasonal_jobs_last_run.txt     (Checkpoint timestamp)      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ seasonal_jobs_last_page.txt    (Pagination checkpoint)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TRANSFORMATION LAYER (transform_seasonal_jobs.py)              ‚îÇ
‚îÇ  ‚úì Column renaming (camelCase ‚Üí snake_case)                     ‚îÇ
‚îÇ  ‚úì Data type casting (dates, numerics)                          ‚îÇ
‚îÇ  ‚úì URL generation for job links                                 ‚îÇ
‚îÇ  ‚úì Output: seasonal_jobs_treated.parquet                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ANALYTICS LAYER (app.py - Streamlit)                           ‚îÇ
‚îÇ  ‚úì Interactive filtering (salary, hours, experience)            ‚îÇ
‚îÇ  ‚úì Data-driven visualizations                                   ‚îÇ
‚îÇ  ‚úì Cached data loading (1 hour TTL)                             ‚îÇ
‚îÇ  ‚úì Responsive web interface                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  End User       ‚îÇ
    ‚îÇ  Dashboard      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
seasonal-jobs-brunss/
‚îÇ
‚îú‚îÄ‚îÄ README.md                              ‚Üê This file
‚îú‚îÄ‚îÄ LICENSE                                ‚Üê Project license
‚îú‚îÄ‚îÄ requirements.txt                       ‚Üê Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ extract_seasonal_jobs.py          ‚Üê Data extraction module
‚îÇ   ‚îú‚îÄ‚îÄ transform_seasonal_jobs.py        ‚Üê Data transformation module
‚îÇ   ‚îî‚îÄ‚îÄ app.py                            ‚Üê Streamlit web application
‚îÇ
‚îî‚îÄ‚îÄ dataset/
    ‚îú‚îÄ‚îÄ seasonal_jobs_raw.parquet         ‚Üê Raw data (cumulative history)
    ‚îú‚îÄ‚îÄ seasonal_jobs_treated.parquet     ‚Üê Cleaned & processed data
    ‚îú‚îÄ‚îÄ seasonal_jobs_last_run.txt        ‚Üê Last extraction timestamp
    ‚îî‚îÄ‚îÄ seasonal_jobs_last_page.txt       ‚Üê Pagination checkpoint (temp)
```

## üîÑ ETL Pipeline Components

### 1. **Extraction Module** (`extract_seasonal_jobs.py`)

**Purpose:** Fetch seasonal job data from the Department of Labor API with intelligent pagination and incremental loading.

**Key Features:**
- Paginated API requests (50 records per page)
- HTTP retry strategy with exponential backoff (max 5 retries)
- Request timeout: 30 seconds
- Active jobs filtering (can be toggled via `FILTER_ACTIVE`)
- Checkpoint-based incremental extraction to avoid re-fetching

**Configuration:**
```python
PAGE_SIZE = 50
REQUEST_TIMEOUT = 30
MAX_RETRIES = 5
BACKOFF_FACTOR = 2
FILTER_ACTIVE = True
```

**Process Flow:**
1. Load last extraction timestamp from checkpoint
2. Paginate through API results
3. Filter new records (timestamp > last checkpoint)
4. Handle deduplication by `case_id`
5. Append to existing raw dataset
6. Update checkpoint for next run

**Output:**
- `seasonal_jobs_raw.parquet` - All raw records (cumulative)
- `seasonal_jobs_last_run.txt` - Latest extraction timestamp (ISO format)

### 2. **Transformation Module** (`transform_seasonal_jobs.py`)

**Purpose:** Clean, standardize, and enrich the raw data for analytics consumption.

**Key Features:**
- Column name normalization (camelCase ‚Üí snake_case)
- Data type casting for dates and numeric fields
- Dynamic URL generation for job listings
- Maintains final column order for consistency

**Column Mapping (Sample):**
- `caseNumber` ‚Üí `case_number`
- `jobTitle` ‚Üí `job_title`
- `basicRateFrom` ‚Üí `basic_rate_from`
- `beginDate` ‚Üí `begin_date`
- `employerBusinessName` ‚Üí `employer_business_name`
- etc. (37 total columns)

**Process Flow:**
1. Load raw Parquet file
2. Rename columns using mapping dictionary
3. Generate job URLs: `https://seasonaljobs.dol.gov/jobs/{case_number}`
4. Export standardized dataset

**Output:**
- `seasonal_jobs_treated.parquet` - Clean, ready-to-analyze data

### 3. **Analytics Application** (`app.py`)

**Purpose:** Interactive web dashboard for exploring seasonal job opportunities.

**Technology Stack:**
- **Streamlit** - Frontend framework
- **Pandas** - Data processing
- **Altair** - Visualizations (via Streamlit)

**Key Features:**
- **Cached Data Loading** - 1-hour TTL for performance
- **Defensive Casting** - Automatic type conversion for numeric and date fields
- **Dynamic Filtering:**
  - Salary range (Basic Rate From)
  - Weekly work hours (Basic)
  - Experience requirements
  - Custom filters expandable
- **Derived Columns:**
  - `begin_month` - Extracted from begin_date
  - `tot_months` - Calculated duration (end_date - begin_date)
- **Wide Layout** - Optimized for desktop viewing

## üì¶ Dependencies

**Core Libraries:**
- `pandas==2.3.3` - Data manipulation
- `streamlit==1.52.2` - Web framework
- `fastparquet==2025.12.0` - Parquet file I/O
- `requests==2.32.5` - HTTP client with retry logic
- `pyarrow==22.0.0` - Arrow format support

**Visualization:**
- `altair==6.0.0` - Declarative visualization grammar

**See `requirements.txt` for complete list**

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- pip or conda

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/seasonal-jobs-brunss.git
   cd seasonal-jobs-brunss
   ```

2. **Create virtual environment (recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### Running the ETL Pipeline

**Step 1: Extract data**
```bash
cd scripts
python extract_seasonal_jobs.py
```

**Step 2: Transform data**
```bash
python transform_seasonal_jobs.py
```

**Step 3: Launch analytics app**
```bash
streamlit run app.py
```

The application will open at `http://localhost:8501`

## üîë Key Configuration Options

### Extraction (`extract_seasonal_jobs.py`)
```python
FILTER_ACTIVE = True    # Only extract active job postings
PAGE_SIZE = 50          # Records per API request
REQUEST_TIMEOUT = 30    # Seconds
MAX_RETRIES = 5         # HTTP retry attempts
BACKOFF_FACTOR = 2      # Exponential backoff multiplier
```

### API Endpoints
- **Main API:** `https://api.seasonaljobs.dol.gov/datahub/?api-version=2020-06-30`
- **Query Example:**
  ```
  GET /datahub/?api-version=2020-06-30&$top=50&$skip=0&$filter=active eq true
  ```

## üìä Data Schema (Treated Dataset)

| Column | Type | Description |
|--------|------|-------------|
| `case_id` | int | Unique identifier |
| `case_number` | str | Case reference number |
| `case_status` | str | Active/Inactive status |
| `visa_class` | str | H2A/H2B/etc. |
| `job_title` | str | Position title |
| `basic_rate_from` | float | Hourly minimum wage |
| `basic_rate_to` | float | Hourly maximum wage |
| `work_hour_num_basic` | int | Weekly hours |
| `begin_date` | datetime | Start date |
| `end_date` | datetime | End date |
| `employer_business_name` | str | Employer name |
| `employer_city` | str | Employer location |
| `employer_state` | str | Employer state |
| `worksite_city` | str | Work location city |
| `worksite_state` | str | Work location state |
| `apply_email` | str | Application email |
| `apply_phone` | str | Application phone |
| `apply_url` | str | Application URL |
| `url_job` | str | Generated job listing URL |
| *...and 18+ more fields* | | |

## ‚öôÔ∏è Technical Highlights

### Incremental Loading Strategy
- **Checkpoint System:** Uses timestamps to track last extraction
- **Benefit:** Only new/modified records are fetched, reducing API calls and processing time
- **Deduplication:** Records with same `case_id` are kept once

### Retry & Resilience
- **HTTP Retries:** Exponential backoff for transient failures (429, 500, 502, 503, 504)
- **Graceful Degradation:** Errors are logged; progress is saved to resume later

### Data Quality
- **Type Casting:** Defensive conversion of numeric/date fields with error coercion
- **Path Handling:** Works correctly in local and CI/CD environments via Path resolution

## üîú Roadmap & Future Enhancements

**Phase 2 - User Features:**
- [ ] OAuth integration (Gmail / Outlook)
- [ ] One-click job application
- [ ] Job recommendations based on user profile
- [ ] Saved searches & alerts

**Phase 3 - Backend Infrastructure:**
- [ ] PostgreSQL database for user data
- [ ] FastAPI backend for scalability
- [ ] RESTful API endpoints
- [ ] Job application tracking

**Phase 4 - Monetization:**
- [ ] Premium filters & analytics
- [ ] Employer dashboard
- [ ] Subscription tiers
- [ ] Automated job matching

## üìù License

This project is licensed under the terms specified in the LICENSE file.

## ü§ù Contributing

Contributions are welcome! Please follow these steps:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ‚ùì FAQ

**Q: How often is data refreshed?**  
A: The extraction runs on a manual schedule. Implement GitHub Actions for automated hourly/daily runs.

**Q: Can I filter by specific states or employers?**  
A: Yes! The Streamlit app supports sidebar filters. Extend `app.py` to add more filter options.

**Q: What if the API is down?**  
A: The retry strategy will attempt 5 times with backoff. If it fails, previous data remains available.

**Q: How is incremental loading tracked?**  
A: A checkpoint file stores the last successful extraction timestamp. The next run fetches only newer records.

## üìû Contact

For questions or support, please open an issue on GitHub.
